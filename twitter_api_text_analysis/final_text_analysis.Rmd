---
title: "Text Analysis for Tweets about Unemployment during Pandemic"
output: html_document
date: '2022-04-13'
---
<hr>
# Overview
 <font size="4"> We extracted tweets using Twitter API to understand how people perceive about the unemployment situation in the United States. We downloaded 500 tweets for three time periods: 2019, 2020, and 2021, querying tweets only from US location and with the term "unemployment" in the tweets. The time interval for data we scrapped for the pre-covid period is from Jan to June 2019, and we selected Jan to June 2020 and July to Dec 2021 as pandemic time periods. We analyzed using three different visualization techniques: word clouds, frequent terms bar chart, and sentiment analysis. </font>

```{r,  echo=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r,  echo=FALSE, message=FALSE}

library(readr)
df <- read_csv("unemployment_tweets_2019.csv")
df2 <- read_csv("unemployment_tweets_2020.csv")
df3 <- read_csv("unemployment_tweets_2021.csv")
```

```{r,  echo=FALSE, message=FALSE}

library(dplyr)
library(tm)
unemployment <- df %>%
  select(id, text) 

colnames(unemployment)[1] <- "doc_id"
colnames(unemployment)[2] <- "text"
unemployment_for_corpus <- unemployment %>%
                        select(doc_id, text)

df_source <- DataframeSource(unemployment_for_corpus)


unemployment2 <- df2 %>%
  select(id, text) 

colnames(unemployment2)[1] <- "doc_id"
colnames(unemployment2)[2] <- "text"
unemployment_for_corpus2 <- unemployment2 %>%
                        select(doc_id, text)

df_source2 <- DataframeSource(unemployment_for_corpus2)


unemployment3 <- df3 %>%
  select(id, text) 

colnames(unemployment3)[1] <- "doc_id"
colnames(unemployment3)[2] <- "text"
unemployment_for_corpus3 <- unemployment3 %>%
                        select(doc_id, text)

df_source3 <- DataframeSource(unemployment_for_corpus3)
```


```{r,  echo=FALSE, message=FALSE, warning =FALSE}
library(ggplot2)
df_corpus_unemployment <- VCorpus(df_source)
#df_corpus_unemployment

df_corpus_unemployment2 <- VCorpus(df_source2)

df_corpus_unemployment3 <- VCorpus(df_source3)
#df_corpus_unemployment

```


```{r,  echo=FALSE, message=FALSE}

#### Text Cleaning: remove certain words
clean_corpus <- function(corpus){
  corpus <- tm_map(corpus, content_transformer(tolower))
  corpus <- tm_map(corpus, removePunctuation)
  corpus <- tm_map(corpus, removeWords, c(stopwords("en")))
  corpus <- tm_map(corpus, removeNumbers)
  corpus <- tm_map(corpus, stripWhitespace)
  corpus <- tm_map(corpus,removeWords, c("unemployment",  "get", "amp", "people", "also", "potus", "realdonaldtrump"))
  return(corpus)
}
df_clean <- clean_corpus(df_corpus_unemployment)

df_clean2 <- clean_corpus(df_corpus_unemployment2)

df_clean3 <- clean_corpus(df_corpus_unemployment3)
```


```{r,  echo=FALSE, message=FALSE}

unemployment_tdm <- TermDocumentMatrix(df_clean)

matrix <- as.matrix(unemployment_tdm)

words <- sort(rowSums(matrix),decreasing=TRUE) 
final_df <- data.frame(word = names(words),freq=words) %>%  filter(word!="jobs") %>% filter(word!="benefits") %>%  filter(word!="realdonaldtrump") %>% filter(word!="covid")

unemployment_tdm2 <- TermDocumentMatrix(df_clean2)

matrix2 <- as.matrix(unemployment_tdm2)

words2 <- sort(rowSums(matrix2),decreasing=TRUE) 
final_df2 <- data.frame(word2 = names(words2),freq2=words2) %>%  filter(word2!="jobs") %>% filter(word2!="benefits") %>%  filter(word2!="realdonaldtrump")


unemployment_tdm3 <- TermDocumentMatrix(df_clean3)

matrix3 <- as.matrix(unemployment_tdm3)

words3 <- sort(rowSums(matrix3),decreasing=TRUE) 
final_df3 <- data.frame(word3 = names(words3),freq3=words3) %>%  filter(word3!="jobs") %>% filter(word3!="benefits") %>%  filter(word3!="realdonaldtrump")
```
<hr>

# Word Clouds
<br>
<font size="4"> From the word clouds, we observed that most frequent words in the pre-covid period were "americans", "will", "millions", and "government". During the pandemic in 2020 and 2021, most frequent words in tweets related to unemployment were "lowest", "economy", "trump", "covid", and "biden". </font>
<br>
<br>
<br>
<font size="4"><center><b> Word Clouds using Unemployment Tweets in 2019, 2020, 2021 respectively. </font></b></center>
<br>
<center>
```{r,  echo=FALSE, message=FALSE, warning =FALSE}
library(wordcloud)

set.seed(1234) 

color <- brewer.pal(10, "GnBu")
# Drop 2 faintest colors
color <- color [-(1:4)]



  
# Create a wordcloud with purple_orange palette
p1 <- wordcloud(words = final_df$word, freq = final_df$freq,
      max.words = 100, colors = color)

```
</center>

<center>
```{r,  echo=FALSE, message=FALSE, warning =FALSE}
p2 <- wordcloud(words = final_df2$word, freq = final_df2$freq,
      max.words = 100, colors = color) 

```
</center>
<center>
```{r,  echo=FALSE, message=FALSE, warning =FALSE}

p3 <- wordcloud(words = final_df3$word, freq = final_df3$freq,
      max.words = 100, colors = color) 

```
</center>
<br>
<hr>
# Word Frequency Visualization
<br>
<font size="4"> Most freuqent terms in unemployment tweets in 2019 were "rate", "economy", "lowest", and "black". This is the result from the pre-covid time, so we could observe that even during the time prior to the pandemic, Americans were not very content with the unemployment situation in the U.S.
<br>
The most frequent terms in 2020 and 2021 were "bill", "covid", "relief", "job", "low", "biden", "back", and "americans". From these selected top words, it seems that people are looking for change and relief. </font>
<br>
<br>
<br>
<font size="4"><center><b> Word Frequency Visualizations in 2019, 2020, 2021 respectively. </font></b></center>
<br>
<center>
```{r,  echo=FALSE, message=FALSE, warning =FALSE}

library(ggplot2)
library(tidytext)
library(wesanderson)

p_1 <- final_df %>%  filter(word!="trump") %>%  filter(word!="realdonaldtrump") %>% 
  filter(freq > 30) %>% anti_join(get_stopwords(source = "smart")) %>%  
  ggplot(.,aes(x=reorder(word, freq), freq ,fill=freq)) +
  coord_flip() +
  labs(title="Most Frequent Terms in Unemployment Related Tweets in 2019", x="Words", y= "Frequency") +
  geom_bar(stat="identity",width=0.6,alpha=1) +
  scale_fill_gradientn(colors=wes_palette(name="Moonrise3")) +
  theme_classic() +
  theme(legend.position = "none")  



p_1
```
</center>

<center>
```{r,  echo=FALSE, message=FALSE, warning =FALSE}

p2_2 <- final_df2 %>%  filter(word2!="trump") %>%  filter(word2!="realdonaldtrump") %>%  filter(word2!="can") %>%  filter(word2!="get")  %>%  filter(word2!="just") %>% filter(word2!="will") %>% filter(word2!="week")%>% 

  filter(freq2 > 35) %>% #anti_join(get_stopwords(source = "smart")) %>%  
  ggplot(.,aes(x=reorder(word2, freq2), freq2 ,fill=freq2)) +
  coord_flip() +
  labs(title="Most Frequent Terms in Unemployment Related Tweets in 2021", x="Words", y= "Frequency") +
  geom_bar(stat="identity",width=0.6,alpha=1) +
  scale_fill_gradientn(colors=wes_palette(name="Moonrise3")) +
  theme_classic() +
  theme(legend.position = "none") 


p2_2
```
</center>

<center>

```{r,  echo=FALSE, message=FALSE, warning =FALSE}

p3_3 <- final_df3 %>%  filter(word3!="trump") %>%  filter(word3!="realdonaldtrump") %>%  filter(word3!="can") %>%  filter(word3!="get")  %>%  filter(word3!="just") %>%  filter(word3!="year") %>% filter(word3!="work") %>% filter(word3!="time") %>% 
  filter(freq3 > 35) %>% #anti_join(get_stopwords(source = "smart")) %>%  
  ggplot(.,aes(x=reorder(word3, freq3), freq3 ,fill=freq3)) +
  coord_flip() +
  labs(title="Most Frequent Terms in Unemployment Related Tweets in 2021", x="Words", y= "Frequency") +
  geom_bar(stat="identity",width=0.6,alpha=1) +
  scale_fill_gradientn(colors=wes_palette(name="Moonrise3")) +
  theme_classic() +
  theme(legend.position = "none") 


p3_3
```

</center>

```{r,  echo=FALSE, message=FALSE, warning =FALSE}
cleaned_twt<-df%>%
  select(text)%>%
  unnest_tokens(word, text)%>%
  anti_join(stop_words)

cleaned_twt2<-df2%>%
  select(text)%>%
  unnest_tokens(word, text)%>%
  anti_join(stop_words)

cleaned_twt3<-df3%>%
  select(text)%>%
  unnest_tokens(word, text)%>%
  anti_join(stop_words)
```
<br>
<br>
<hr>
# Sentiment Analysis
<br>

<font size="4"> We performed sentiment analysis using the Bing lexicon. It seems that in 2019, the proportion of positive and negative words appeared are equally distributed. Although the number of counts of negative words appeared are somewhat greater than that of positive words, the it seems that there were more variety of positive sentiments appeared in 2019 tweets, contributing to the total counts of positive words in 2019 tweets. 
<br>
During the pandemic, more words with negative sentiment appeared in unemployment related tweets, which is evident in the below graph. As the pandemic started and the economy went down, the unemployment tweets had about twice more negative words.  </font>
<br>
<br>
<br>
<hr>

<center>
```{r,  echo=FALSE, message=FALSE, warning =FALSE}

library(textdata)
senti <- get_sentiments("bing") %>% filter(word!="trump")

bing <- cleaned_twt%>% 
  inner_join(senti)%>%
  count(word,sentiment, sort=TRUE)%>% 
  filter(sentiment!="trump")   %>% filter(word!="benefits") %>%
  ungroup()

bing%>%
  group_by(sentiment)%>%
  mutate(word=reorder(word,n))%>%
  top_n(10)%>%
  ggplot(aes(word,n,fill=sentiment))+
  geom_col(show.legend=FALSE)+
  facet_wrap(~sentiment,scale="free_y")+
  labs(title='Setiment Analysis on Unemployment Tweets in 2019',
       y='Contribution to Sentiment')+
  coord_flip()+
  theme_classic() 

```

```{r,  echo=FALSE, message=FALSE, warning =FALSE}

senti2 <- get_sentiments("bing") %>% filter(word!="trump")

bing2 <- cleaned_twt2%>% 
  inner_join(senti2)%>%
  count(word,sentiment, sort=TRUE)%>% 
  filter(sentiment!="trump")   %>% filter(word!="benefits") %>%
  ungroup()

bing2%>%
  group_by(sentiment)%>%
  mutate(word=reorder(word,n))%>%
  top_n(10)%>%
  ggplot(aes(word,n,fill=sentiment))+
  geom_col(show.legend=FALSE)+
  facet_wrap(~sentiment,scale="free_y")+
  labs(title='Setiment Analysis on Unemployment Tweets in 2020',
       y='Contribution to Sentiment')+
  coord_flip()+
  theme_classic() 

```

```{r,  echo=FALSE, message=FALSE, warning =FALSE}

bing3 <- cleaned_twt3%>% 
  inner_join(senti)%>%
  count(word,sentiment, sort=TRUE)%>% 
  filter(sentiment!="trump")   %>% filter(word!="benefits") %>%
  ungroup()

bing3%>%
  group_by(sentiment)%>%
  mutate(word=reorder(word,n))%>%
  top_n(10)%>%
  ggplot(aes(word,n,fill=sentiment))+
  geom_col(show.legend=FALSE)+
  facet_wrap(~sentiment,scale="free_y")+
  labs(title='Setiment Analysis on Unemployment Tweets in 2021',
       y='Contribution to Sentiment')+
  coord_flip()+
  theme_classic() 
```
</center><br>
<br>
<hr>
# Most Frequent Positive and Negative Words in Tweets
<br>
<font size="4"> From the word clouds below, it seems that the key words in each year seem to be "booming", "relief", and "recovery". From these keywords, we could assume the message people had about the employment situation during the time. In 2019, people were concerned on economic reform and how to improve the employment rate and decrease poverty. 
<br> Once the pandemic started, in 2020, people were concerned about (job) loss, as the words "lost", "lapsed", "losing", "lose" occured frequently. In 2021, from the result, we could suppose that the economic situation is getting better compared to the prior year, as more positive words appeared, such as "recovery", "releif", "support", "enhanced", "happy", "easy", and "promise".  </font>
<br>
<br>
<center>
<font size="4"><b> Sentiment Analysis using Word Cloud for 2019, 2020, 2021 respectively. </font></b>

```{r,  echo=FALSE, message=FALSE, warning =FALSE}
library(reshape2)
senti.bing <- df %>%
              unnest_tokens(word, text) %>%                 # split text into words
              anti_join(stop_words, by = "word") %>%                  # remove stop words
             # filter(!grepl('[0-9]', word !="trump")) %>% 
              filter(word!="trump") %>% filter(word!="benefits") %>%
  # remove numbers
              inner_join(get_sentiments("bing"), by = "word") 
            
            
            senti.bing%>%
              count(word, sentiment, sort = TRUE) %>%
              acast(word ~ sentiment, value.var = "n", fill = 0) %>%  #convert tibble into matrix
              comparison.cloud(colors = c("red", "blue"),
                               max.words = 200)
            
```

```{r,  echo=FALSE, message=FALSE, warning =FALSE}
senti.bing2 <- df2 %>%
              unnest_tokens(word, text) %>%                
              anti_join(stop_words, by = "word") %>%       
              filter(word!="trump") %>% filter(word!="benefits") %>%
              inner_join(get_sentiments("bing"), by = "word") 
            
            
            senti.bing2%>%
              count(word, sentiment, sort = TRUE) %>%
              acast(word ~ sentiment, value.var = "n", fill = 0) %>%  #convert tibble into matrix
              comparison.cloud(colors = c("red", "blue"),
                               max.words = 200)
```

```{r,  echo=FALSE, message=FALSE, warning =FALSE}
senti.bing3 <- df3 %>%
              unnest_tokens(word, text) %>%                
              anti_join(stop_words, by = "word") %>%       
              filter(word!="trump") %>% filter(word!="benefits") %>%
              inner_join(get_sentiments("bing"), by = "word") 
            
            
            senti.bing3%>%
              count(word, sentiment, sort = TRUE) %>%
              acast(word ~ sentiment, value.var = "n", fill = 0) %>% 
              comparison.cloud(colors = c("red", "blue"),
                               max.words = 200)
```

</center>