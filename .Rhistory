unemployment2 <- df2 %>%
select(id, text)
colnames(unemployment2)[1] <- "doc_id"
colnames(unemployment2)[2] <- "text"
unemployment_for_corpus2 <- unemployment2 %>%
select(doc_id, text)
df_source2 <- DataframeSource(unemployment_for_corpus2)
unemployment3 <- df3 %>%
select(id, text)
colnames(unemployment3)[1] <- "doc_id"
colnames(unemployment3)[2] <- "text"
unemployment_for_corpus3 <- unemployment3 %>%
select(doc_id, text)
df_source3 <- DataframeSource(unemployment_for_corpus3)
# Making tweet corpora
df_corpus_unemployment <- VCorpus(df_source)
df_corpus_unemployment2 <- VCorpus(df_source2)
df_corpus_unemployment3 <- VCorpus(df_source3)
# Text cleaning, removing certain words
clean_corpus <- function(corpus){
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeWords, c(stopwords("en")))
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus,removeWords, c("unemployment",  "get", "amp",
"people", "also", "potus",
"realdonaldtrump"))
return(corpus)
}
df_clean <- clean_corpus(df_corpus_unemployment)
df_clean2 <- clean_corpus(df_corpus_unemployment2)
df_clean3 <- clean_corpus(df_corpus_unemployment3)
# Text Mining
unemployment_tdm <- TermDocumentMatrix(df_clean)
matrix <- as.matrix(unemployment_tdm)
words <- sort(rowSums(matrix),decreasing=TRUE)
final_df <- data.frame(word = names(words),freq=words) %>%
filter(word!="jobs") %>% filter(word!="benefits") %>%
filter(word!="realdonaldtrump") %>%
filter(word!="covid")
unemployment_tdm2 <- TermDocumentMatrix(df_clean2)
matrix2 <- as.matrix(unemployment_tdm2)
words2 <- sort(rowSums(matrix2),decreasing=TRUE)
final_df2 <- data.frame(word2 = names(words2),freq2=words2) %>%
filter(word2!="jobs") %>%
filter(word2!="benefits") %>%
filter(word2!="realdonaldtrump")
unemployment_tdm3 <- TermDocumentMatrix(df_clean3)
matrix3 <- as.matrix(unemployment_tdm3)
words3 <- sort(rowSums(matrix3),decreasing=TRUE)
final_df3 <- data.frame(word3 = names(words3),freq3=words3) %>%
filter(word3!="jobs") %>% filter(word3!="benefits") %>%
filter(word3!="realdonaldtrump")
# For sentiment analyses
cleaned_twt<-df%>%
select(text)%>%
unnest_tokens(word, text)%>%
anti_join(stop_words)
cleaned_twt2<-df2%>%
select(text)%>%
unnest_tokens(word, text)%>%
anti_join(stop_words)
cleaned_twt3<-df3%>%
select(text)%>%
unnest_tokens(word, text)%>%
anti_join(stop_words)
##### Toby's work: Movement range maps------------------------------------------
## Importing usa dataframe
usa <- read.csv("movement_range_maps/movement_range_data_usa_2020.txt", sep = ",")
# selecting years of interest
usa_2020 <- usa %>%
filter(year == 2020)
## Importing the shape files
nation_shp <- readOGR("movement_range_maps/cb_2021_us_nation_5m/cb_2021_us_nation_5m.shp")
county_shp <- readOGR("movement_range_maps/cb_2021_us_county_500k/cb_2021_us_county_500k.shp")
state_shp <- readOGR("movement_range_maps/cb_2021_us_state_500k/cb_2021_us_state_500k.shp")
# source the ui and server files
source("ui.R")
source("server.R")
# launch the app
shinyApp(ui = ui, server = server)
# Importing packages
library(ggplot2)
library(plotly)
library(tidyverse)
library(zoo)
library(dplyr)
library(lubridate)
library(devtools)
library(leaflet)
library(readr)
library(rgdal)
library(tmap)
library(ggmap)
library(RColorBrewer)
library(tm)
library(wordcloud)
library(tidytext)
library(wesanderson)
library(textdata)
library(reshape2)
library(graphics)
library(stopwords)
library(stringr)
library(shiny)
# Data Pre-processing protocols
##### Wenyi's work: Weekly wage and working hours plots-------------------------
# Importing datasets
weekly_wage <- read.csv("./avg_weekly_wage_hours/wage4.csv")
weekly_hours <- read.csv("./avg_weekly_wage_hours/hours4.csv")
#####---------------------------------------------------------------------------
##### Nicole's work: Unemployment rates (national, industrial and stage averages)
## Importing datasets
total <- read.csv("unemployment/SeriesReport-20220423235636_21d8c2.csv",
stringsAsFactors = FALSE,
fileEncoding="UTF-8-BOM")
states <- read.csv("unemployment/unemployment by states.csv",
stringsAsFactors = FALSE,
fileEncoding="UTF-8-BOM")
industry <- read.csv("unemployment/Unempolyment rate by industry.csv",
stringsAsFactors = FALSE,
fileEncoding="UTF-8-BOM")
# Pre-processing steps for leaflet map
colnames(states)[2] <- 'Rate_2019'
colnames(states)[3] <- 'Rate_2020'
colnames(states)[4] <- 'Rate_2021'
geom <- read.csv("unemployment/statelatlong.csv")
colnames(geom)[4] <- 'State'
geom <- subset(geom, select = -c(1))
statesgeom <- states %>%
left_join(geom, by = 'State')
# Setting the labels
content3 <- paste("unemployment Rate:",statesgeom$Rate_2019,"<br/>",
"State:",statesgeom$State,"<br/>")
content4 <- paste("unemployment Rate:",statesgeom$Rate_2020,"<br/>",
"State",statesgeom$State,"<br/>")
content5 <- paste("unemployment Rate:",statesgeom$Rate_2021,"<br/>",
"State",statesgeom$State,"<br/>")
##### Shauna's work: # Twitter Text Mining and Sentiment Analysis---------------
# Importing and cleaning data
df <- read_csv("twitter_api_text_analysis/unemployment_tweets_2019.csv")
df2 <- read_csv("twitter_api_text_analysis/unemployment_tweets_2020.csv")
df3 <- read_csv("twitter_api_text_analysis/unemployment_tweets_2021.csv")
unemployment <- df %>%
select(id, text)
colnames(unemployment)[1] <- "doc_id"
colnames(unemployment)[2] <- "text"
unemployment_for_corpus <- unemployment %>%
select(doc_id, text)
df_source <- DataframeSource(unemployment_for_corpus)
unemployment2 <- df2 %>%
select(id, text)
colnames(unemployment2)[1] <- "doc_id"
colnames(unemployment2)[2] <- "text"
unemployment_for_corpus2 <- unemployment2 %>%
select(doc_id, text)
df_source2 <- DataframeSource(unemployment_for_corpus2)
unemployment3 <- df3 %>%
select(id, text)
colnames(unemployment3)[1] <- "doc_id"
colnames(unemployment3)[2] <- "text"
unemployment_for_corpus3 <- unemployment3 %>%
select(doc_id, text)
df_source3 <- DataframeSource(unemployment_for_corpus3)
# Making tweet corpora
df_corpus_unemployment <- VCorpus(df_source)
df_corpus_unemployment2 <- VCorpus(df_source2)
df_corpus_unemployment3 <- VCorpus(df_source3)
# Text cleaning, removing certain words
clean_corpus <- function(corpus){
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeWords, c(stopwords("en")))
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus,removeWords, c("unemployment",  "get", "amp",
"people", "also", "potus",
"realdonaldtrump"))
return(corpus)
}
df_clean <- clean_corpus(df_corpus_unemployment)
df_clean2 <- clean_corpus(df_corpus_unemployment2)
df_clean3 <- clean_corpus(df_corpus_unemployment3)
# Text Mining
unemployment_tdm <- TermDocumentMatrix(df_clean)
matrix <- as.matrix(unemployment_tdm)
words <- sort(rowSums(matrix),decreasing=TRUE)
final_df <- data.frame(word = names(words),freq=words) %>%
filter(word!="jobs") %>% filter(word!="benefits") %>%
filter(word!="realdonaldtrump") %>%
filter(word!="covid")
unemployment_tdm2 <- TermDocumentMatrix(df_clean2)
matrix2 <- as.matrix(unemployment_tdm2)
words2 <- sort(rowSums(matrix2),decreasing=TRUE)
final_df2 <- data.frame(word2 = names(words2),freq2=words2) %>%
filter(word2!="jobs") %>%
filter(word2!="benefits") %>%
filter(word2!="realdonaldtrump")
unemployment_tdm3 <- TermDocumentMatrix(df_clean3)
matrix3 <- as.matrix(unemployment_tdm3)
words3 <- sort(rowSums(matrix3),decreasing=TRUE)
final_df3 <- data.frame(word3 = names(words3),freq3=words3) %>%
filter(word3!="jobs") %>% filter(word3!="benefits") %>%
filter(word3!="realdonaldtrump")
# For sentiment analyses
cleaned_twt<-df%>%
select(text)%>%
unnest_tokens(word, text)%>%
anti_join(stop_words)
cleaned_twt2<-df2%>%
select(text)%>%
unnest_tokens(word, text)%>%
anti_join(stop_words)
cleaned_twt3<-df3%>%
select(text)%>%
unnest_tokens(word, text)%>%
anti_join(stop_words)
##### Toby's work: Movement range maps------------------------------------------
## Importing usa dataframe
usa <- read.csv("movement_range_maps/movement_range_data_usa_2020.txt", sep = ",")
# selecting years of interest
usa_2020 <- usa %>%
filter(year == 2020)
## Importing the shape files
nation_shp <- readOGR("movement_range_maps/cb_2021_us_nation_5m/cb_2021_us_nation_5m.shp")
county_shp <- readOGR("movement_range_maps/cb_2021_us_county_500k/cb_2021_us_county_500k.shp")
state_shp <- readOGR("movement_range_maps/cb_2021_us_state_500k/cb_2021_us_state_500k.shp")
# source the ui and server files
source("ui.R")
source("server.R")
# launch the app
shinyApp(ui = ui, server = server)
# source the ui and server files
source("ui.R")
source("server.R")
# launch the app
shinyApp(ui = ui, server = server)
install.packages('rsconnect')
install.packages("rsconnect")
rsconnect::setAccountInfo(name='hnhs75-toby-law',
token='CA700EE30191EF88A37DA632F47440E4',
secret='<SECRET>')
rsconnect::setAccountInfo(name='hnhs75-toby-law',
token='CA700EE30191EF88A37DA632F47440E4',
secret='<SECRET>')
rsconnect::setAccountInfo(name='hnhs75-toby-law',
token='CA700EE30191EF88A37DA632F47440E4',
secret='Xs2mAGm5SFXEEm11G8p91t5IFYR9/qhXwNdMCcQm')
library(rsconnect)
rsconnect::setAccountInfo(name='hnhs75-toby-law',
token='CA700EE30191EF88A37DA632F47440E4',
secret='Xs2mAGm5SFXEEm11G8p91t5IFYR9/qhXwNdMCcQm')
setwd("C:/Users/User/Desktop/Data Visualization/Group_A_COVID-19_insights")
# Importing packages
library(ggplot2)
library(plotly)
library(tidyverse)
library(zoo)
library(dplyr)
library(lubridate)
library(devtools)
library(leaflet)
library(readr)
library(rgdal)
library(tmap)
library(ggmap)
library(RColorBrewer)
library(tm)
library(wordcloud)
library(tidytext)
library(wesanderson)
library(textdata)
library(reshape2)
library(graphics)
library(stopwords)
library(stringr)
library(shiny)
# Data Pre-processing protocols
##### Wenyi's work: Weekly wage and working hours plots-------------------------
# Importing datasets
weekly_wage <- read.csv("./avg_weekly_wage_hours/wage4.csv")
weekly_hours <- read.csv("./avg_weekly_wage_hours/hours4.csv")
#####---------------------------------------------------------------------------
##### Nicole's work: Unemployment rates (national, industrial and stage averages)
## Importing datasets
total <- read.csv("unemployment/SeriesReport-20220423235636_21d8c2.csv",
stringsAsFactors = FALSE,
fileEncoding="UTF-8-BOM")
states <- read.csv("unemployment/unemployment by states.csv",
stringsAsFactors = FALSE,
fileEncoding="UTF-8-BOM")
industry <- read.csv("unemployment/Unempolyment rate by industry.csv",
stringsAsFactors = FALSE,
fileEncoding="UTF-8-BOM")
# Pre-processing steps for leaflet map
colnames(states)[2] <- 'Rate_2019'
colnames(states)[3] <- 'Rate_2020'
colnames(states)[4] <- 'Rate_2021'
geom <- read.csv("unemployment/statelatlong.csv")
colnames(geom)[4] <- 'State'
geom <- subset(geom, select = -c(1))
statesgeom <- states %>%
left_join(geom, by = 'State')
# Setting the labels
content3 <- paste("unemployment Rate:",statesgeom$Rate_2019,"<br/>",
"State:",statesgeom$State,"<br/>")
content4 <- paste("unemployment Rate:",statesgeom$Rate_2020,"<br/>",
"State",statesgeom$State,"<br/>")
content5 <- paste("unemployment Rate:",statesgeom$Rate_2021,"<br/>",
"State",statesgeom$State,"<br/>")
##### Shauna's work: # Twitter Text Mining and Sentiment Analysis---------------
# Importing and cleaning data
df <- read_csv("twitter_api_text_analysis/unemployment_tweets_2019.csv")
df2 <- read_csv("twitter_api_text_analysis/unemployment_tweets_2020.csv")
df3 <- read_csv("twitter_api_text_analysis/unemployment_tweets_2021.csv")
unemployment <- df %>%
select(id, text)
colnames(unemployment)[1] <- "doc_id"
colnames(unemployment)[2] <- "text"
unemployment_for_corpus <- unemployment %>%
select(doc_id, text)
df_source <- DataframeSource(unemployment_for_corpus)
unemployment2 <- df2 %>%
select(id, text)
colnames(unemployment2)[1] <- "doc_id"
colnames(unemployment2)[2] <- "text"
unemployment_for_corpus2 <- unemployment2 %>%
select(doc_id, text)
df_source2 <- DataframeSource(unemployment_for_corpus2)
unemployment3 <- df3 %>%
select(id, text)
colnames(unemployment3)[1] <- "doc_id"
colnames(unemployment3)[2] <- "text"
unemployment_for_corpus3 <- unemployment3 %>%
select(doc_id, text)
df_source3 <- DataframeSource(unemployment_for_corpus3)
# Making tweet corpora
df_corpus_unemployment <- VCorpus(df_source)
df_corpus_unemployment2 <- VCorpus(df_source2)
df_corpus_unemployment3 <- VCorpus(df_source3)
# Text cleaning, removing certain words
clean_corpus <- function(corpus){
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeWords, c(stopwords("en")))
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus,removeWords, c("unemployment",  "get", "amp",
"people", "also", "potus",
"realdonaldtrump"))
return(corpus)
}
df_clean <- clean_corpus(df_corpus_unemployment)
df_clean2 <- clean_corpus(df_corpus_unemployment2)
df_clean3 <- clean_corpus(df_corpus_unemployment3)
# Text Mining
unemployment_tdm <- TermDocumentMatrix(df_clean)
matrix <- as.matrix(unemployment_tdm)
words <- sort(rowSums(matrix),decreasing=TRUE)
final_df <- data.frame(word = names(words),freq=words) %>%
filter(word!="jobs") %>% filter(word!="benefits") %>%
filter(word!="realdonaldtrump") %>%
filter(word!="covid")
unemployment_tdm2 <- TermDocumentMatrix(df_clean2)
matrix2 <- as.matrix(unemployment_tdm2)
words2 <- sort(rowSums(matrix2),decreasing=TRUE)
final_df2 <- data.frame(word2 = names(words2),freq2=words2) %>%
filter(word2!="jobs") %>%
filter(word2!="benefits") %>%
filter(word2!="realdonaldtrump")
unemployment_tdm3 <- TermDocumentMatrix(df_clean3)
matrix3 <- as.matrix(unemployment_tdm3)
words3 <- sort(rowSums(matrix3),decreasing=TRUE)
final_df3 <- data.frame(word3 = names(words3),freq3=words3) %>%
filter(word3!="jobs") %>% filter(word3!="benefits") %>%
filter(word3!="realdonaldtrump")
# For sentiment analyses
cleaned_twt<-df%>%
select(text)%>%
unnest_tokens(word, text)%>%
anti_join(stop_words)
cleaned_twt2<-df2%>%
select(text)%>%
unnest_tokens(word, text)%>%
anti_join(stop_words)
cleaned_twt3<-df3%>%
select(text)%>%
unnest_tokens(word, text)%>%
anti_join(stop_words)
##### Toby's work: Movement range maps------------------------------------------
## Importing usa dataframe
usa <- read.csv("movement_range_maps/movement_range_data_usa_2020.txt", sep = ",")
# selecting years of interest
usa_2020 <- usa %>%
filter(year == 2020)
## Importing the shape files
nation_shp <- readOGR("movement_range_maps/cb_2021_us_nation_5m/cb_2021_us_nation_5m.shp")
county_shp <- readOGR("movement_range_maps/cb_2021_us_county_500k/cb_2021_us_county_500k.shp")
state_shp <- readOGR("movement_range_maps/cb_2021_us_state_500k/cb_2021_us_state_500k.shp")
# source the ui and server files
source("ui.R")
source("server.R")
# launch the app
shinyApp(ui = ui, server = server)
# source the ui and server files
source("ui.R")
source("server.R")
# launch the app
shinyApp(ui = ui, server = server)
shiny::runApp()
shiny::runApp()
install.packages("shinythemes")
runApp()
shiny::runApp()
runApp()
View(total)
knitr::opts_chunk$set(echo = TRUE, fig.align="center", warning = FALSE, message = FALSE)
library(ggplot2)
library(tidyverse)
library(zoo)
library(dplyr)
library(lubridate)
library(plotly)
library(devtools)
library(leaflet)
library(readr)
library(rgdal)
library(tmap)
library(ggmap)
library(RColorBrewer)
library(htmltools)
total <- read.csv("/Users/yuwang/Documents/GitHub/gr5063/Group_A_COVID-19_insights/umployment rate/SeriesReport-20220423235636_21d8c2.csv", stringsAsFactors = FALSE)
total <- read.csv("/Users/yuwang/Documents/GitHub/gr5063/Group_A_COVID-19_insights/umployment/SeriesReport-20220423235636_21d8c2.csv", stringsAsFactors = FALSE)
total <- read.csv("/Users/yuwang/Documents/GitHub/gr5063/Group_A_COVID-19_insights/unemployment/SeriesReport-20220423235636_21d8c2.csv", stringsAsFactors = FALSE)
states <- read.csv("/Users/yuwang/Documents/GitHub/gr5063/Group_A_COVID-19_insights/unemployment/unemployment by states.csv", stringsAsFactors = FALSE)
industry <- read.csv("/Users/yuwang/Documents/GitHub/gr5063/Group_A_COVID-19_insights/unemployment/Unempolyment rate by industry.csv", stringsAsFactors = FALSE)
total1 <- total %>%
gather("id", "value", 2:13) %>%
filter(id == "Jun"|id == "Dec", Year >= 2016 & Year != 2022)
total1$month <- match(total1[, 2], month.abb)
total1$Date <- paste(total1$Year,total1$month)
total1$Date <- ym(total1$Date)
total1 <- total1 %>%
arrange(Date)
total1$value1 <- paste0(as.matrix(total1$value), "%")
plot1 <- plot_ly(total1, x = ~Date, y = ~value1, type = 'scatter', mode = 'lines+markers', text = ~value) %>%
layout(
xaxis = list(showgrid = FALSE, title = "Year"),
yaxis = list(title = "Unemployment Rate %")
)  %>%
add_text(text = ~value1, textposition = "top center",
showlegend = F) %>%
add_trace(data = total1[total1$Date == 2020.], x = ~Date, y = ~value1)
plot1_centered <- div(plot1, align = "center")
plot1_centered
plot1 <- plot_ly(total1, x = ~Date, y = ~value1, type = 'scatter', mode = 'lines+markers', text = ~value) %>%
layout(
xaxis = list(showgrid = FALSE, title = "Year"),
yaxis = list(title = "Unemployment Rate %")
)  %>%
add_text(text = ~value1, textposition = "top center",
showlegend = F)
View(total1)
plot1 <- plot_ly(total1, x = ~Date, y = ~value1, type = 'scatter', mode = 'lines+markers', text = ~value) %>%
layout(
xaxis = list(showgrid = FALSE, title = "Year"),
yaxis = list(title = "Unemployment Rate %")
)  %>%
add_text(text = ~value1, textposition = "top center",
showlegend = F) %>%
add_trace(data = total1[total1$Date == 2020-06-01], x = ~Date, y = ~value1, mode = 'markers')
plot1 <- plot_ly(total1, x = ~Date, y = ~value1, type = 'scatter', mode = 'lines+markers', text = ~value) %>%
layout(
xaxis = list(showgrid = FALSE, title = "Year"),
yaxis = list(title = "Unemployment Rate %")
)  %>%
add_text(text = ~value1, textposition = "top center",
showlegend = F) %>%
add_trace(data = total1[total1$Date == 2020-06-01], x = ~Date, y = ~value1, mode = 'markers')
plot1
plot1 <- plot_ly(total1, x = ~Date, y = ~value1, type = 'scatter', mode = 'lines+markers', text = ~value) %>%
layout(
xaxis = list(showgrid = FALSE, title = "Year"),
yaxis = list(title = "Unemployment Rate %")
)  %>%
add_text(text = ~value1, textposition = "top center",
showlegend = F) %>%
add_trace(data = total1[total1$Date == 2020-06-01], mode = 'markers')
plot1
plot1 <- plot_ly(total1, x = ~Date, y = ~value1, type = 'scatter', mode = 'lines+markers', text = ~value) %>%
layout(
xaxis = list(showgrid = FALSE, title = "Year"),
yaxis = list(title = "Unemployment Rate %")
)  %>%
add_text(text = ~value1, textposition = "top center",
showlegend = F)
plot1
plot1 <- plot_ly(total1, x = ~Date, y = ~value1, type = 'scatter', mode = 'lines+markers', text = ~value) %>%
layout(
xaxis = list(showgrid = FALSE, title = "Year"),
yaxis = list(title = "Unemployment Rate %")
)
plot1
plot1 <- plot_ly(total1, x = ~Date, y = ~value, type = 'scatter', mode = 'lines+markers', text = ~value) %>%
layout(
xaxis = list(showgrid = FALSE, title = "Year"),
yaxis = list(title = "Unemployment Rate %")
)
plot1
runApp()
runApp()
